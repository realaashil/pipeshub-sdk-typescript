/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import { crawlingJobsGetStatus } from "../funcs/crawling-jobs-get-status.js";
import { crawlingJobsList } from "../funcs/crawling-jobs-list.js";
import { crawlingJobsPause } from "../funcs/crawling-jobs-pause.js";
import { crawlingJobsRemoveAll } from "../funcs/crawling-jobs-remove-all.js";
import { crawlingJobsRemove } from "../funcs/crawling-jobs-remove.js";
import { crawlingJobsResume } from "../funcs/crawling-jobs-resume.js";
import { crawlingJobsSchedule } from "../funcs/crawling-jobs-schedule.js";
import { ClientSDK, RequestOptions } from "../lib/sdks.js";
import * as operations from "../models/operations/index.js";
import { unwrapAsync } from "../types/fp.js";

export class CrawlingJobs extends ClientSDK {
  /**
   * Schedule a crawling job
   *
   * @remarks
   * Schedule a new crawling job for a specific connector instance.<br><br>
   *
   * <b>Overview:</b><br>
   * Creates a scheduled crawling job that will sync data from the specified connector into
   * PipesHub's search index. The job is added to a BullMQ queue and will execute according
   * to the specified schedule configuration.<br><br>
   *
   * <b>Schedule Types:</b><br>
   * <ul>
   * <li><b>hourly:</b> Run every X hours at specified minute (e.g., every 2 hours at :30)</li>
   * <li><b>daily:</b> Run once per day at specified time (e.g., 2:00 AM daily)</li>
   * <li><b>weekly:</b> Run on specific days of the week (e.g., Mon/Wed/Fri at 3:00 AM)</li>
   * <li><b>monthly:</b> Run on specific day of month (e.g., 1st of each month at 4:00 AM)</li>
   * <li><b>custom:</b> Use cron expression for complex schedules</li>
   * <li><b>once:</b> Run once at a specific future datetime</li>
   * </ul>
   *
   * <b>Access Control:</b><br>
   * <ul>
   * <li>Team-scoped connectors: Requires admin privileges</li>
   * <li>Personal-scoped connectors: Only the creator can schedule jobs</li>
   * </ul>
   *
   * <b>Job Behavior:</b><br>
   * <ul>
   * <li>If a job already exists for this connector, it will be replaced</li>
   * <li>Disabled schedules (<code>isEnabled: false</code>) will throw an error</li>
   * <li>Jobs use exponential backoff for retries (5s, 10s, 20s, etc.)</li>
   * <li>Only last 10 completed/failed jobs are retained per connector</li>
   * </ul>
   *
   * <b>Related Endpoints:</b><br>
   * <ul>
   * <li><code>GET /crawlingManager/{connector}/{connectorId}/schedule</code> - Get job status</li>
   * <li><code>POST /crawlingManager/{connector}/{connectorId}/pause</code> - Pause job</li>
   * <li><code>DELETE /crawlingManager/{connector}/{connectorId}/remove</code> - Remove job</li>
   * </ul>
   */
  async schedule(
    request: operations.ScheduleCrawlingJobRequest,
    options?: RequestOptions,
  ): Promise<operations.ScheduleCrawlingJobResponse> {
    return unwrapAsync(crawlingJobsSchedule(
      this,
      request,
      options,
    ));
  }

  /**
   * Get crawling job status
   *
   * @remarks
   * Retrieve the current status of a scheduled crawling job for a specific connector.<br><br>
   *
   * <b>Overview:</b><br>
   * Returns detailed information about the most recent crawling job for the specified connector,
   * including its current state, progress, timing information, and any error details.<br><br>
   *
   * <b>Job States:</b><br>
   * <ul>
   * <li><b>waiting:</b> Job is queued and waiting to be processed</li>
   * <li><b>active:</b> Job is currently being processed by a worker</li>
   * <li><b>completed:</b> Job finished successfully</li>
   * <li><b>failed:</b> Job failed after exhausting retry attempts</li>
   * <li><b>delayed:</b> Job is scheduled for future execution</li>
   * <li><b>paused:</b> Job has been manually paused</li>
   * </ul>
   *
   * <b>Access Control:</b><br>
   * Same as scheduling - team connectors require admin, personal connectors require creator.
   */
  async getStatus(
    request: operations.GetCrawlingJobStatusRequest,
    options?: RequestOptions,
  ): Promise<operations.GetCrawlingJobStatusResponse> {
    return unwrapAsync(crawlingJobsGetStatus(
      this,
      request,
      options,
    ));
  }

  /**
   * Remove a crawling job
   *
   * @remarks
   * Permanently remove a scheduled crawling job for a specific connector.<br><br>
   *
   * <b>Overview:</b><br>
   * Removes the crawling job and all associated data from the queue. This includes
   * removing repeatable job configurations and cleaning up job history.<br><br>
   *
   * <b>What Gets Removed:</b><br>
   * <ul>
   * <li>Active or waiting job instances</li>
   * <li>Repeatable job configuration (for recurring schedules)</li>
   * <li>Paused job information</li>
   * <li>Job mappings and metadata</li>
   * </ul>
   *
   * <b>Note:</b> Completed and failed job records may be retained for audit purposes.
   *
   * <b>Related Endpoints:</b><br>
   * <ul>
   * <li><code>DELETE /crawlingManager/schedule/all</code> - Remove all jobs for organization</li>
   * </ul>
   */
  async remove(
    request: operations.RemoveCrawlingJobRequest,
    options?: RequestOptions,
  ): Promise<operations.RemoveCrawlingJobResponse> {
    return unwrapAsync(crawlingJobsRemove(
      this,
      request,
      options,
    ));
  }

  /**
   * Pause a crawling job
   *
   * @remarks
   * Pause a running or scheduled crawling job without losing its configuration.<br><br>
   *
   * <b>Overview:</b><br>
   * Pausing a job stores its complete configuration and removes it from the active queue.
   * The job can be resumed later with <code>POST /crawlingManager/{connector}/{connectorId}/resume</code>,
   * which will restore the exact same schedule configuration.<br><br>
   *
   * <b>How Pausing Works:</b><br>
   * <ol>
   * <li>Current job configuration is stored in memory</li>
   * <li>Active/repeatable job is removed from BullMQ queue</li>
   * <li>Job state changes to "paused"</li>
   * <li>No new job executions will occur until resumed</li>
   * </ol>
   *
   * <b>Use Cases:</b><br>
   * <ul>
   * <li>Temporarily stop crawling during maintenance</li>
   * <li>Pause data sync while investigating issues</li>
   * <li>Stop crawling for a connector being reconfigured</li>
   * </ul>
   *
   * <b>Note:</b> If a job is currently active (processing), it will complete before pausing.
   */
  async pause(
    request: operations.PauseCrawlingJobRequest,
    options?: RequestOptions,
  ): Promise<operations.PauseCrawlingJobResponse> {
    return unwrapAsync(crawlingJobsPause(
      this,
      request,
      options,
    ));
  }

  /**
   * Resume a paused crawling job
   *
   * @remarks
   * Resume a previously paused crawling job using its stored configuration.<br><br>
   *
   * <b>Overview:</b><br>
   * Restores a paused job to active state using the exact configuration it had when paused.
   * A new job is created in BullMQ with the same schedule settings.<br><br>
   *
   * <b>How Resuming Works:</b><br>
   * <ol>
   * <li>Retrieve stored job configuration from pause state</li>
   * <li>Create new scheduled job with same configuration</li>
   * <li>Remove from paused jobs tracking</li>
   * <li>Job will execute according to its original schedule</li>
   * </ol>
   *
   * <b>Note:</b> The job will resume according to its schedule, not immediately execute
   * (unless it's a one-time job that hasn't run yet).
   */
  async resume(
    request: operations.ResumeCrawlingJobRequest,
    options?: RequestOptions,
  ): Promise<operations.ResumeCrawlingJobResponse> {
    return unwrapAsync(crawlingJobsResume(
      this,
      request,
      options,
    ));
  }

  /**
   * Get all crawling job statuses
   *
   * @remarks
   * Retrieve the status of all scheduled crawling jobs for the current organization.<br><br>
   *
   * <b>Overview:</b><br>
   * Returns a list of all crawling jobs across all connectors for the authenticated user's
   * organization. This includes active, waiting, paused, completed, and failed jobs.<br><br>
   *
   * <b>Response Details:</b><br>
   * <ul>
   * <li>Jobs are grouped by connector type</li>
   * <li>Last 10 jobs per connector type are returned</li>
   * <li>Includes both active queue jobs and paused jobs</li>
   * </ul>
   *
   * <b>Use Cases:</b><br>
   * <ul>
   * <li>Dashboard overview of all crawling activities</li>
   * <li>Monitoring job health across connectors</li>
   * <li>Identifying failed or stuck jobs</li>
   * </ul>
   */
  async list(
    options?: RequestOptions,
  ): Promise<operations.GetAllCrawlingJobStatusResponse> {
    return unwrapAsync(crawlingJobsList(
      this,
      options,
    ));
  }

  /**
   * Remove all crawling jobs
   *
   * @remarks
   * Remove all scheduled crawling jobs for the current organization.<br><br>
   *
   * <b>Overview:</b><br>
   * Bulk operation to remove all crawling jobs across all connectors for the organization.
   * This is useful when decommissioning an organization or doing a complete reset.<br><br>
   *
   * <b>What Gets Removed:</b><br>
   * <ul>
   * <li>All active and waiting jobs</li>
   * <li>All repeatable job configurations</li>
   * <li>All paused jobs</li>
   * <li>All job mappings for the organization</li>
   * </ul>
   *
   * <b>Warning:</b> This operation cannot be undone. All job configurations will need
   * to be recreated manually.
   */
  async removeAll(
    options?: RequestOptions,
  ): Promise<operations.RemoveAllCrawlingJobsResponse> {
    return unwrapAsync(crawlingJobsRemoveAll(
      this,
      options,
    ));
  }
}
