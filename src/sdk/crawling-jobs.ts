/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import { crawlingJobsGetCrawlingJobStatus } from "../funcs/crawling-jobs-get-crawling-job-status.js";
import { crawlingJobsRemoveCrawlingJob } from "../funcs/crawling-jobs-remove-crawling-job.js";
import { crawlingJobsScheduleCrawlingJob } from "../funcs/crawling-jobs-schedule-crawling-job.js";
import { ClientSDK, RequestOptions } from "../lib/sdks.js";
import * as operations from "../models/operations/index.js";
import { unwrapAsync } from "../types/fp.js";

export class CrawlingJobs extends ClientSDK {
  /**
   * Schedule a crawling job
   *
   * @remarks
   * Schedule a new crawling job for a specific connector instance.<br><br>
   *
   * <b>Overview:</b><br>
   * Creates a scheduled crawling job that will sync data from the specified connector into
   * PipesHub's search index. The job is added to a BullMQ queue and will execute according
   * to the specified schedule configuration.<br><br>
   *
   * <b>Schedule Types:</b><br>
   * <ul>
   * <li><b>hourly:</b> Run every X hours at specified minute (e.g., every 2 hours at :30)</li>
   * <li><b>daily:</b> Run once per day at specified time (e.g., 2:00 AM daily)</li>
   * <li><b>weekly:</b> Run on specific days of the week (e.g., Mon/Wed/Fri at 3:00 AM)</li>
   * <li><b>monthly:</b> Run on specific day of month (e.g., 1st of each month at 4:00 AM)</li>
   * <li><b>custom:</b> Use cron expression for complex schedules</li>
   * <li><b>once:</b> Run once at a specific future datetime</li>
   * </ul>
   *
   * <b>Access Control:</b><br>
   * <ul>
   * <li>Team-scoped connectors: Requires admin privileges</li>
   * <li>Personal-scoped connectors: Only the creator can schedule jobs</li>
   * </ul>
   *
   * <b>Job Behavior:</b><br>
   * <ul>
   * <li>If a job already exists for this connector, it will be replaced</li>
   * <li>Disabled schedules (<code>isEnabled: false</code>) will throw an error</li>
   * <li>Jobs use exponential backoff for retries (5s, 10s, 20s, etc.)</li>
   * <li>Only last 10 completed/failed jobs are retained per connector</li>
   * </ul>
   *
   * <b>Related Endpoints:</b><br>
   * <ul>
   * <li><code>GET /crawlingManager/{connector}/{connectorId}/schedule</code> - Get job status</li>
   * <li><code>POST /crawlingManager/{connector}/{connectorId}/pause</code> - Pause job</li>
   * <li><code>DELETE /crawlingManager/{connector}/{connectorId}/remove</code> - Remove job</li>
   * </ul>
   */
  async scheduleCrawlingJob(
    request: operations.ScheduleCrawlingJobRequest,
    options?: RequestOptions,
  ): Promise<operations.ScheduleCrawlingJobResponse> {
    return unwrapAsync(crawlingJobsScheduleCrawlingJob(
      this,
      request,
      options,
    ));
  }

  /**
   * Get crawling job status
   *
   * @remarks
   * Retrieve the current status of a scheduled crawling job for a specific connector.<br><br>
   *
   * <b>Overview:</b><br>
   * Returns detailed information about the most recent crawling job for the specified connector,
   * including its current state, progress, timing information, and any error details.<br><br>
   *
   * <b>Job States:</b><br>
   * <ul>
   * <li><b>waiting:</b> Job is queued and waiting to be processed</li>
   * <li><b>active:</b> Job is currently being processed by a worker</li>
   * <li><b>completed:</b> Job finished successfully</li>
   * <li><b>failed:</b> Job failed after exhausting retry attempts</li>
   * <li><b>delayed:</b> Job is scheduled for future execution</li>
   * <li><b>paused:</b> Job has been manually paused</li>
   * </ul>
   *
   * <b>Access Control:</b><br>
   * Same as scheduling - team connectors require admin, personal connectors require creator.
   */
  async getCrawlingJobStatus(
    request: operations.GetCrawlingJobStatusRequest,
    options?: RequestOptions,
  ): Promise<operations.GetCrawlingJobStatusResponse> {
    return unwrapAsync(crawlingJobsGetCrawlingJobStatus(
      this,
      request,
      options,
    ));
  }

  /**
   * Remove a crawling job
   *
   * @remarks
   * Permanently remove a scheduled crawling job for a specific connector.<br><br>
   *
   * <b>Overview:</b><br>
   * Removes the crawling job and all associated data from the queue. This includes
   * removing repeatable job configurations and cleaning up job history.<br><br>
   *
   * <b>What Gets Removed:</b><br>
   * <ul>
   * <li>Active or waiting job instances</li>
   * <li>Repeatable job configuration (for recurring schedules)</li>
   * <li>Paused job information</li>
   * <li>Job mappings and metadata</li>
   * </ul>
   *
   * <b>Note:</b> Completed and failed job records may be retained for audit purposes.
   *
   * <b>Related Endpoints:</b><br>
   * <ul>
   * <li><code>DELETE /crawlingManager/schedule/all</code> - Remove all jobs for organization</li>
   * </ul>
   */
  async removeCrawlingJob(
    request: operations.RemoveCrawlingJobRequest,
    options?: RequestOptions,
  ): Promise<operations.RemoveCrawlingJobResponse> {
    return unwrapAsync(crawlingJobsRemoveCrawlingJob(
      this,
      request,
      options,
    ));
  }
}
