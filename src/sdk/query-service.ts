/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import { queryServiceChatStream } from "../funcs/query-service-chat-stream.js";
import { queryServiceChat } from "../funcs/query-service-chat.js";
import { queryServiceHealthCheck } from "../funcs/query-service-health-check.js";
import { queryServiceListTools } from "../funcs/query-service-list-tools.js";
import { queryServiceSearch } from "../funcs/query-service-search.js";
import { ClientSDK, RequestOptions } from "../lib/sdks.js";
import * as operations from "../models/operations/index.js";
import { unwrapAsync } from "../types/fp.js";

export class QueryService extends ClientSDK {
  /**
   * [Query Service] Semantic search
   *
   * @remarks
   * ⚠️ <b>INTERNAL SERVICE ENDPOINT</b><br><br>
   *
   * Perform semantic search across indexed documents using vector embeddings.<br><br>
   *
   * <b>Service:</b> Query Service<br>
   * <b>Port:</b> 8000<br>
   * <b>Base URL:</b> <code>http://localhost:8000</code><br><br>
   *
   * <b>Authentication:</b> Requires user JWT token (proxied from main API) or scoped service token<br><br>
   *
   * <b>How It Works:</b><br>
   * <ol>
   * <li>Query is transformed and expanded using LLM</li>
   * <li>Embeddings are generated for search queries</li>
   * <li>Vector similarity search in Qdrant</li>
   * <li>Results filtered by user permissions</li>
   * <li>Optional knowledge base filtering</li>
   * </ol>
   */
  async search(
    request: operations.QueryServiceSearchRequest,
    options?: RequestOptions,
  ): Promise<operations.QueryServiceSearchResponse> {
    return unwrapAsync(queryServiceSearch(
      this,
      request,
      options,
    ));
  }

  /**
   * [Query Service] Chat with knowledge base
   *
   * @remarks
   * ⚠️ <b>INTERNAL SERVICE ENDPOINT</b><br><br>
   *
   * Conversational AI endpoint with RAG (Retrieval-Augmented Generation).<br><br>
   *
   * <b>Service:</b> Query Service<br>
   * <b>Port:</b> 8000<br>
   * <b>Base URL:</b> <code>http://localhost:8000</code><br><br>
   *
   * <b>Authentication:</b> Requires user JWT token (proxied from main API) or scoped service token<br><br>
   *
   * <b>Features:</b><br>
   * <ul>
   * <li>Multi-turn conversation support</li>
   * <li>Context from knowledge base</li>
   * <li>Citation of source documents</li>
   * <li>Multiple chat modes (quick, analysis, deep_research, creative, precise)</li>
   * <li>Multi-model support (OpenAI, Anthropic, Ollama, etc.)</li>
   * </ul>
   */
  async chat(
    request: operations.QueryServiceChatRequest,
    options?: RequestOptions,
  ): Promise<void> {
    return unwrapAsync(queryServiceChat(
      this,
      request,
      options,
    ));
  }

  /**
   * [Query Service] Streaming chat with knowledge base
   *
   * @remarks
   * ⚠️ <b>INTERNAL SERVICE ENDPOINT</b><br><br>
   *
   * Streaming conversational AI endpoint with real-time token delivery.<br><br>
   *
   * <b>Service:</b> Query Service<br>
   * <b>Port:</b> 8000<br>
   * <b>Base URL:</b> <code>http://localhost:8000</code><br><br>
   *
   * <b>Authentication:</b> Requires user JWT token (proxied from main API) or scoped service token<br><br>
   *
   * <b>SSE Events:</b><br>
   * <ul>
   * <li><code>status</code>: Processing status updates</li>
   * <li><code>chunk</code>: Token/text chunks</li>
   * <li><code>citations</code>: Source citations</li>
   * <li><code>done</code>: Stream complete</li>
   * <li><code>error</code>: Error occurred</li>
   * </ul>
   */
  async chatStream(
    options?: RequestOptions,
  ): Promise<string> {
    return unwrapAsync(queryServiceChatStream(
      this,
      options,
    ));
  }

  /**
   * [Query Service] AI model health check
   *
   * @remarks
   * ⚠️ <b>INTERNAL SERVICE ENDPOINT</b><br><br>
   *
   * Validate LLM or embedding model configuration.<br><br>
   *
   * <b>Service:</b> Query Service<br>
   * <b>Port:</b> 8000<br>
   * <b>Base URL:</b> <code>http://localhost:8000</code><br><br>
   *
   * <b>Authentication:</b> Requires scoped service token<br><br>
   *
   * <b>Model Types:</b>
   * <ul>
   * <li><code>llm</code> - Test LLM configuration (text generation)</li>
   * <li><code>embedding</code> - Test embedding model configuration</li>
   * </ul>
   */
  async healthCheck(
    security: operations.QueryServiceHealthCheckSecurity,
    request: operations.QueryServiceHealthCheckRequest,
    options?: RequestOptions,
  ): Promise<operations.QueryServiceHealthCheckResponse> {
    return unwrapAsync(queryServiceHealthCheck(
      this,
      security,
      request,
      options,
    ));
  }

  /**
   * [Query Service] List available agent tools
   *
   * @remarks
   * Retrieve all available tools that can be used by AI agents.<br><br>
   *
   * <b>Service:</b> Query Service<br>
   * <b>Port:</b> 8000<br>
   * <b>Base URL:</b> <code>http://localhost:8000</code><br><br>
   *
   * <b>Overview:</b><br>
   * Returns a list of tools registered in the system, including their parameters,
   * descriptions, and tags. Tools are loaded from ArangoDB.<br><br>
   *
   * <b>Use Cases:</b><br>
   * <ul>
   * <li>Agent configuration UI - show available tools to assign to agents</li>
   * <li>Tool discovery for building custom agent workflows</li>
   * </ul>
   */
  async listTools(
    options?: RequestOptions,
  ): Promise<Array<operations.QueryListToolsResponse>> {
    return unwrapAsync(queryServiceListTools(
      this,
      options,
    ));
  }
}
