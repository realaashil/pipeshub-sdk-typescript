/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v4-mini";
import { PipeshubCore } from "../core.js";
import { encodeJSON, encodeSimple } from "../lib/encodings.js";
import * as M from "../lib/matchers.js";
import { compactMap } from "../lib/primitives.js";
import { safeParse } from "../lib/schemas.js";
import { RequestOptions } from "../lib/sdks.js";
import { extractSecurity, resolveGlobalSecurity } from "../lib/security.js";
import { pathToFunc } from "../lib/url.js";
import {
  ConnectionError,
  InvalidRequestError,
  RequestAbortedError,
  RequestTimeoutError,
  UnexpectedClientError,
} from "../models/errors/http-client-errors.js";
import { PipeshubError } from "../models/errors/pipeshub-error.js";
import { ResponseValidationError } from "../models/errors/response-validation-error.js";
import { SDKValidationError } from "../models/errors/sdk-validation-error.js";
import * as operations from "../models/operations/index.js";
import { APICall, APIPromise } from "../types/async.js";
import { Result } from "../types/fp.js";

/**
 * Schedule a crawling job
 *
 * @remarks
 * Schedule a new crawling job for a specific connector instance.<br><br>
 *
 * <b>Overview:</b><br>
 * Creates a scheduled crawling job that will sync data from the specified connector into
 * PipesHub's search index. The job is added to a BullMQ queue and will execute according
 * to the specified schedule configuration.<br><br>
 *
 * <b>Schedule Types:</b><br>
 * <ul>
 * <li><b>hourly:</b> Run every X hours at specified minute (e.g., every 2 hours at :30)</li>
 * <li><b>daily:</b> Run once per day at specified time (e.g., 2:00 AM daily)</li>
 * <li><b>weekly:</b> Run on specific days of the week (e.g., Mon/Wed/Fri at 3:00 AM)</li>
 * <li><b>monthly:</b> Run on specific day of month (e.g., 1st of each month at 4:00 AM)</li>
 * <li><b>custom:</b> Use cron expression for complex schedules</li>
 * <li><b>once:</b> Run once at a specific future datetime</li>
 * </ul>
 *
 * <b>Access Control:</b><br>
 * <ul>
 * <li>Team-scoped connectors: Requires admin privileges</li>
 * <li>Personal-scoped connectors: Only the creator can schedule jobs</li>
 * </ul>
 *
 * <b>Job Behavior:</b><br>
 * <ul>
 * <li>If a job already exists for this connector, it will be replaced</li>
 * <li>Disabled schedules (<code>isEnabled: false</code>) will throw an error</li>
 * <li>Jobs use exponential backoff for retries (5s, 10s, 20s, etc.)</li>
 * <li>Only last 10 completed/failed jobs are retained per connector</li>
 * </ul>
 *
 * <b>Related Endpoints:</b><br>
 * <ul>
 * <li><code>GET /crawlingManager/{connector}/{connectorId}/schedule</code> - Get job status</li>
 * <li><code>POST /crawlingManager/{connector}/{connectorId}/pause</code> - Pause job</li>
 * <li><code>DELETE /crawlingManager/{connector}/{connectorId}/remove</code> - Remove job</li>
 * </ul>
 */
export function crawlingJobsSchedule(
  client: PipeshubCore,
  request: operations.ScheduleCrawlingJobRequest,
  options?: RequestOptions,
): APIPromise<
  Result<
    operations.ScheduleCrawlingJobResponse,
    | PipeshubError
    | ResponseValidationError
    | ConnectionError
    | RequestAbortedError
    | RequestTimeoutError
    | InvalidRequestError
    | UnexpectedClientError
    | SDKValidationError
  >
> {
  return new APIPromise($do(
    client,
    request,
    options,
  ));
}

async function $do(
  client: PipeshubCore,
  request: operations.ScheduleCrawlingJobRequest,
  options?: RequestOptions,
): Promise<
  [
    Result<
      operations.ScheduleCrawlingJobResponse,
      | PipeshubError
      | ResponseValidationError
      | ConnectionError
      | RequestAbortedError
      | RequestTimeoutError
      | InvalidRequestError
      | UnexpectedClientError
      | SDKValidationError
    >,
    APICall,
  ]
> {
  const parsed = safeParse(
    request,
    (value) =>
      z.parse(operations.ScheduleCrawlingJobRequest$outboundSchema, value),
    "Input validation failed",
  );
  if (!parsed.ok) {
    return [parsed, { status: "invalid" }];
  }
  const payload = parsed.value;
  const body = encodeJSON("body", payload.body, { explode: true });

  const pathParams = {
    connector: encodeSimple("connector", payload.connector, {
      explode: false,
      charEncoding: "percent",
    }),
    connectorId: encodeSimple("connectorId", payload.connectorId, {
      explode: false,
      charEncoding: "percent",
    }),
  };

  const path = pathToFunc(
    "/crawlingManager/{connector}/{connectorId}/schedule",
  )(pathParams);

  const headers = new Headers(compactMap({
    "Content-Type": "application/json",
    Accept: "application/json",
  }));

  const secConfig = await extractSecurity(client._options.bearerAuth);
  const securityInput = secConfig == null ? {} : { bearerAuth: secConfig };
  const requestSecurity = resolveGlobalSecurity(securityInput);

  const context = {
    options: client._options,
    baseURL: options?.serverURL ?? client._baseURL ?? "",
    operationID: "scheduleCrawlingJob",
    oAuth2Scopes: null,

    resolvedSecurity: requestSecurity,

    securitySource: client._options.bearerAuth,
    retryConfig: options?.retries
      || client._options.retryConfig
      || { strategy: "none" },
    retryCodes: options?.retryCodes || ["429", "500", "502", "503", "504"],
  };

  const requestRes = client._createRequest(context, {
    security: requestSecurity,
    method: "POST",
    baseURL: options?.serverURL,
    path: path,
    headers: headers,
    body: body,
    userAgent: client._options.userAgent,
    timeoutMs: options?.timeoutMs || client._options.timeoutMs || -1,
  }, options);
  if (!requestRes.ok) {
    return [requestRes, { status: "invalid" }];
  }
  const req = requestRes.value;

  const doResult = await client._do(req, {
    context,
    errorCodes: ["400", "401", "403", "404", "4XX", "5XX"],
    retryConfig: context.retryConfig,
    retryCodes: context.retryCodes,
  });
  if (!doResult.ok) {
    return [doResult, { status: "request-error", request: req }];
  }
  const response = doResult.value;

  const [result] = await M.match<
    operations.ScheduleCrawlingJobResponse,
    | PipeshubError
    | ResponseValidationError
    | ConnectionError
    | RequestAbortedError
    | RequestTimeoutError
    | InvalidRequestError
    | UnexpectedClientError
    | SDKValidationError
  >(
    M.json(201, operations.ScheduleCrawlingJobResponse$inboundSchema),
    M.fail([400, 401, 403, 404, "4XX"]),
    M.fail("5XX"),
  )(response, req);
  if (!result.ok) {
    return [result, { status: "complete", request: req, response }];
  }

  return [result, { status: "complete", request: req, response }];
}
